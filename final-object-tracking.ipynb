{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12479089,"sourceType":"datasetVersion","datasetId":7873789}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nfrom ultralytics import YOLO\nimport subprocess\nimport sys\nimport torchvision.transforms as transforms\nfrom collections import OrderedDict, defaultdict, deque\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom scipy.spatial.distance import cdist\nfrom scipy.optimize import linear_sum_assignment\nfrom filterpy.kalman import KalmanFilter\nfrom torchvision.ops import nms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:04:26.012240Z","iopub.execute_input":"2025-07-17T06:04:26.012928Z","iopub.status.idle":"2025-07-17T06:04:26.017764Z","shell.execute_reply.started":"2025-07-17T06:04:26.012896Z","shell.execute_reply":"2025-07-17T06:04:26.016869Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Install required packages\ndef install_requirements():\n    \"\"\"Install required packages for object tracking\"\"\"\n    packages = [\n        'ultralytics',\n        'opencv-python',\n        'torch',\n        'torchvision',\n        'torchaudio',\n        'scipy',\n        'filterpy',\n        'lap',\n        'cython-bbox'\n    ]\n    \n    for package in packages:\n        try:\n            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n            print(f\"✓ {package} installed successfully\")\n        except subprocess.CalledProcessError:\n            print(f\"✗ Failed to install {package}\")\n\n# Install requirements\ninstall_requirements()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleFeatureExtractor:\n    \"\"\"Simplified CNN feature extractor for re-identification\"\"\"\n    \n    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n        self.device = device\n        \n        # Use a simpler ResNet-based feature extractor\n        self.model = models.resnet50(pretrained=True)\n        self.model.fc = nn.Identity()  # Remove final classification layer\n        self.model.to(self.device)\n        self.model.eval()\n        \n        # Standard ImageNet preprocessing\n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                               std=[0.229, 0.224, 0.225])\n        ])\n        \n    def extract_features(self, image_crops):\n        \"\"\"Extract features from image crops\"\"\"\n        if len(image_crops) == 0:\n            return np.array([])\n        \n        features = []\n        with torch.no_grad():\n            for crop in image_crops:\n                if crop.size == 0:\n                    continue\n                    \n                try:\n                    # Ensure crop is valid\n                    if crop.shape[0] < 10 or crop.shape[1] < 10:\n                        continue\n                        \n                    # Convert BGR to RGB\n                    crop_rgb = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n                    \n                    # Transform and extract features\n                    crop_tensor = self.transform(crop_rgb).unsqueeze(0).to(self.device)\n                    feature = self.model(crop_tensor)\n                    features.append(feature.cpu().numpy().flatten())\n                except Exception as e:\n                    print(f\"Error processing crop: {e}\")\n                    continue\n        \n        return np.array(features) if features else np.array([])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:04:54.452141Z","iopub.execute_input":"2025-07-17T06:04:54.452423Z","iopub.status.idle":"2025-07-17T06:04:54.460034Z","shell.execute_reply.started":"2025-07-17T06:04:54.452402Z","shell.execute_reply":"2025-07-17T06:04:54.459404Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class KalmanBoxTracker:\n    \"\"\"Kalman filter tracker for bounding boxes with enhanced features\"\"\"\n    count = 0\n    \n    def __init__(self, bbox, class_id=0, conf=0.0):\n        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n        self.kf.F = np.array([\n            [1, 0, 0, 0, 1, 0, 0],\n            [0, 1, 0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0, 0, 1],\n            [0, 0, 0, 1, 0, 0, 0],\n            [0, 0, 0, 0, 1, 0, 0],\n            [0, 0, 0, 0, 0, 1, 0],\n            [0, 0, 0, 0, 0, 0, 1]\n        ])\n        \n        self.kf.H = np.array([\n            [1, 0, 0, 0, 0, 0, 0],\n            [0, 1, 0, 0, 0, 0, 0],\n            [0, 0, 1, 0, 0, 0, 0],\n            [0, 0, 0, 1, 0, 0, 0]\n        ])\n        \n        # Tuned noise parameters\n        self.kf.R[2:, 2:] *= 5.\n        self.kf.P[4:, 4:] *= 500.\n        self.kf.P *= 5.\n        self.kf.Q[-1, -1] *= 0.05\n        self.kf.Q[4:, 4:] *= 0.05\n        \n        self.kf.x[:4] = self.convert_bbox_to_z(bbox)\n        self.time_since_update = 0\n        self.id = KalmanBoxTracker.count\n        KalmanBoxTracker.count += 1\n        self.history = []\n        self.hits = 0\n        self.hit_streak = 0\n        self.age = 0\n        self.class_id = class_id\n        self.conf = conf\n        \n        # Enhanced feature management\n        self.feature_history = deque(maxlen=10)  # Store last 10 features\n        self.max_similarity = 0.8  # Threshold for confident matching\n        \n    def update(self, bbox, class_id=0, conf=0.0, feature=None):\n        self.time_since_update = 0\n        self.history = []\n        self.hits += 1\n        self.hit_streak += 1\n        self.class_id = class_id\n        self.conf = max(self.conf, conf)  # Keep highest confidence\n        self.kf.update(self.convert_bbox_to_z(bbox))\n        \n        # Store feature for re-identification\n        if feature is not None:\n            self.feature_history.append(feature)\n    \n    def predict(self):\n        if (self.kf.x[6] + self.kf.x[2]) <= 0:\n            self.kf.x[6] *= 0.0\n        self.kf.predict()\n        self.age += 1\n        if self.time_since_update > 0:\n            self.hit_streak = 0\n        self.time_since_update += 1\n        self.history.append(self.convert_x_to_bbox(self.kf.x))\n        return self.history[-1]\n    \n    def get_state(self):\n        return self.convert_x_to_bbox(self.kf.x)\n    \n    def get_average_feature(self):\n        if len(self.feature_history) == 0:\n            return None\n        return np.mean(self.feature_history, axis=0)\n    \n    def get_most_recent_feature(self):\n        if len(self.feature_history) == 0:\n            return None\n        return self.feature_history[-1]\n    \n    @staticmethod\n    def convert_bbox_to_z(bbox):\n        w = bbox[2] - bbox[0]\n        h = bbox[3] - bbox[1]\n        x = bbox[0] + w/2.\n        y = bbox[1] + h/2.\n        s = w * h\n        r = w / float(h)\n        return np.array([x, y, s, r]).reshape((4, 1))\n    \n    @staticmethod\n    def convert_x_to_bbox(x, score=None):\n        w = np.sqrt(x[2] * x[3])\n        h = x[2] / w\n        if score is None:\n            return np.array([x[0] - w/2., x[1] - h/2., x[0] + w/2., x[1] + h/2.]).reshape((1, 4))\n        else:\n            return np.array([x[0] - w/2., x[1] - h/2., x[0] + w/2., x[1] + h/2., score]).reshape((1, 5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:04:54.460781Z","iopub.execute_input":"2025-07-17T06:04:54.460955Z","iopub.status.idle":"2025-07-17T06:04:54.484787Z","shell.execute_reply.started":"2025-07-17T06:04:54.460940Z","shell.execute_reply":"2025-07-17T06:04:54.484181Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class DeepOCSORT:\n    \"\"\"Enhanced DeepOCSORT tracker with ResNet features\"\"\"\n    \n    def __init__(self, max_age=30, min_hits=3, iou_threshold=0.4, feature_threshold=0.7):\n        self.max_age = max_age\n        self.min_hits = min_hits\n        self.iou_threshold = iou_threshold\n        self.feature_threshold = feature_threshold\n        self.trackers = []\n        self.frame_count = 0\n        self.temporal_window = 3  # Number of frames to consider for feature matching\n        \n        # Initialize feature extractor\n        try:\n            self.feature_extractor = SimpleFeatureExtractor()\n            print(\"✓ Feature extractor initialized successfully\")\n        except Exception as e:\n            print(f\"⚠ Feature extractor initialization failed: {e}\")\n            self.feature_extractor = None\n        \n    def update(self, detections, frame):\n        \"\"\"Update tracker with detections\"\"\"\n        self.frame_count += 1\n        \n        # Extract features from detection crops\n        features = self._extract_features(detections, frame) if self.feature_extractor else []\n        \n        # Predict existing trackers\n        trks = np.zeros((len(self.trackers), 5))\n        to_del = []\n        \n        for t, trk in enumerate(self.trackers):\n            pos = trk.predict()[0]\n            trks[t] = [pos[0], pos[1], pos[2], pos[3], 0]\n            if np.any(np.isnan(pos)):\n                to_del.append(t)\n        \n        trks = np.ma.compress_rows(np.ma.masked_invalid(trks))\n        for t in reversed(to_del):\n            self.trackers.pop(t)\n        \n        # Associate detections to trackers\n        if len(detections) > 0:\n            dets = np.array([[d[0], d[1], d[2], d[3], d[4]] for d in detections])\n        else:\n            dets = np.empty((0, 5))\n        \n        matched, unmatched_dets, unmatched_trks = self._associate_detections_to_trackers(\n            dets, trks, features, self.iou_threshold)\n        \n        # Update matched trackers\n        for m in matched:\n            det_idx, trk_idx = m[0], m[1]\n            feature = features[det_idx] if det_idx < len(features) else None\n            self.trackers[trk_idx].update(\n                dets[det_idx, :4], \n                int(detections[det_idx][5]), \n                dets[det_idx, 4],\n                feature\n            )\n        \n        # Create new trackers for unmatched detections\n        for i in unmatched_dets:\n            feature = features[i] if i < len(features) else None\n            trk = KalmanBoxTracker(dets[i, :4], int(detections[i][5]), dets[i, 4])\n            if feature is not None:\n                trk.update(dets[i, :4], int(detections[i][5]), dets[i, 4], feature)\n            self.trackers.append(trk)\n        \n        # Return confirmed tracks\n        tracks = []\n        for trk in self.trackers:\n            if (trk.time_since_update < 1) and (trk.hit_streak >= self.min_hits or self.frame_count <= self.min_hits):\n                d = trk.get_state()[0]\n                tracks.append({\n                    'id': trk.id,\n                    'bbox': [d[0], d[1], d[2], d[3]],\n                    'class_id': trk.class_id,\n                    'conf': trk.conf\n                })\n        \n        # Remove dead trackers\n        i = len(self.trackers)\n        for trk in reversed(self.trackers):\n            if trk.time_since_update > self.max_age:\n                self.trackers.pop(i - 1)\n            i -= 1\n        \n        return tracks\n    \n    def _extract_features(self, detections, frame):\n        \"\"\"Extract features from detection crops with bounds checking\"\"\"\n        if not self.feature_extractor:\n            return []\n            \n        crops = []\n        for det in detections:\n            x1, y1, x2, y2 = map(int, det[:4])\n            # Add bounds checking\n            x1 = max(0, x1)\n            y1 = max(0, y1)\n            x2 = min(frame.shape[1], x2)\n            y2 = min(frame.shape[0], y2)\n            \n            if x2 > x1 and y2 > y1:  # Valid crop\n                crop = frame[y1:y2, x1:x2]\n                crops.append(crop)\n            else:\n                crops.append(np.array([]))  # Empty crop\n        \n        if len(crops) == 0:\n            return []\n        \n        try:\n            features = self.feature_extractor.extract_features(crops)\n            return features\n        except Exception as e:\n            print(f\"Feature extraction error: {e}\")\n            return []\n    \n    def _associate_detections_to_trackers(self, detections, trackers, features, iou_threshold=0.3):\n        \"\"\"Enhanced association with both IoU and appearance features\"\"\"\n        if len(trackers) == 0:\n            return np.empty((0, 2), dtype=int), np.arange(len(detections)), np.empty((0, 5), dtype=int)\n        \n        # IoU matching\n        iou_matrix = np.zeros((len(detections), len(trackers)))\n        for d, det in enumerate(detections):\n            for t, trk in enumerate(trackers):\n                iou_matrix[d, t] = self._iou(det, trk)\n        \n        # Appearance matching using cosine similarity\n        appearance_matrix = np.zeros((len(detections), len(trackers)))\n        if len(features) > 0 and self.feature_extractor:\n            for d, det_feature in enumerate(features):\n                for t, tracker in enumerate(self.trackers):\n                    avg_feature = tracker.get_average_feature()\n                    if avg_feature is not None:\n                        try:\n                            # Use cosine similarity\n                            similarity = 1 - cdist(\n                                det_feature.reshape(1, -1), \n                                avg_feature.reshape(1, -1), \n                                'cosine'\n                            )[0][0]\n                            appearance_matrix[d, t] = similarity\n                        except Exception as e:\n                            appearance_matrix[d, t] = 0\n        \n        # Combine metrics with adaptive weighting\n        combined_matrix = np.zeros_like(iou_matrix)\n        for d in range(len(detections)):\n            for t in range(len(trackers)):\n                if iou_matrix[d, t] > 0.1:  # Some overlap exists\n                    combined_matrix[d, t] = 0.6 * iou_matrix[d, t] + 0.4 * appearance_matrix[d, t]\n                else:  # No overlap, rely more on appearance\n                    combined_matrix[d, t] = 0.2 * iou_matrix[d, t] + 0.8 * appearance_matrix[d, t]\n        \n        # Solve assignment problem\n        if min(combined_matrix.shape) > 0:\n            row_ind, col_ind = linear_sum_assignment(-combined_matrix)\n            matched_indices = np.stack([row_ind, col_ind], axis=1)\n        else:\n            matched_indices = np.empty(shape=(0, 2))\n        \n        unmatched_detections = []\n        for d, det in enumerate(detections):\n            if d not in matched_indices[:, 0]:\n                unmatched_detections.append(d)\n        \n        unmatched_trackers = []\n        for t, trk in enumerate(trackers):\n            if t not in matched_indices[:, 1]:\n                unmatched_trackers.append(t)\n        \n        matches = []\n        for m in matched_indices:\n            # Only confirm matches with sufficient evidence\n            if (combined_matrix[m[0], m[1]] < self.iou_threshold and \n                appearance_matrix[m[0], m[1]] < self.feature_threshold):\n                unmatched_detections.append(m[0])\n                unmatched_trackers.append(m[1])\n            else:\n                matches.append(m.reshape(1, 2))\n        \n        if len(matches) == 0:\n            matches = np.empty((0, 2), dtype=int)\n        else:\n            matches = np.concatenate(matches, axis=0)\n        \n        return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n    \n    def _iou(self, bb_test, bb_gt):\n        \"\"\"Calculate IoU between two bounding boxes\"\"\"\n        xx1 = np.maximum(bb_test[0], bb_gt[0])\n        yy1 = np.maximum(bb_test[1], bb_gt[1])\n        xx2 = np.minimum(bb_test[2], bb_gt[2])\n        yy2 = np.minimum(bb_test[3], bb_gt[3])\n        w = np.maximum(0., xx2 - xx1)\n        h = np.maximum(0., yy2 - yy1)\n        wh = w * h\n        o = wh / ((bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1]) + \n                  (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1]) - wh)\n        return o","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:05:02.077728Z","iopub.execute_input":"2025-07-17T06:05:02.078013Z","iopub.status.idle":"2025-07-17T06:05:02.102138Z","shell.execute_reply.started":"2025-07-17T06:05:02.077990Z","shell.execute_reply":"2025-07-17T06:05:02.101401Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class ObjectTracker:\n    \"\"\"Enhanced Object Tracker with YOLOv8 + DeepOCSORT + ResNet features\"\"\"\n    \n    def __init__(self, model_path='yolov8x.pt', conf_threshold=0.25, iou_threshold=0.45):\n        self.conf_threshold = conf_threshold\n        self.iou_threshold = iou_threshold\n        \n        # Load YOLOv8 model\n        print(\"Loading YOLOv8 model...\")\n        try:\n            self.model = YOLO(model_path)\n            print(f\"✓ Model loaded successfully. Device: {self.model.device}\")\n        except Exception as e:\n            print(f\"✗ Model loading failed: {e}\")\n            raise\n        \n        # Initialize DeepOCSORT tracker\n        print(\"Initializing DeepOCSORT tracker...\")\n        self.tracker = DeepOCSORT(\n            max_age=30,\n            min_hits=3,\n            iou_threshold=0.2,\n            feature_threshold=0.7\n        )\n        self.class_names = [\n            'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck',\n            'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench',\n            'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra',\n            'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n            'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n            'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',\n            'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n            'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n            'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse',\n            'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n            'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n            'toothbrush'\n        ]\n        \n        # Color palette for visualization\n        self.colors = np.random.randint(0, 255, size=(200, 3), dtype=np.uint8)\n        \n        # Tracking metrics\n        self.total_detections = 0\n        self.total_frames = 0\n\n    def detect_objects(self, frame):\n        \"\"\"Enhanced object detection with error handling\"\"\"\n        try:\n            # Run YOLO detection\n            results = self.model(frame, conf=self.conf_threshold, iou=self.iou_threshold, verbose=False)\n            detections = []\n            \n            for result in results:\n                if result.boxes is not None:\n                    # Convert to numpy arrays\n                    boxes = result.boxes.xyxy.cpu().numpy()\n                    scores = result.boxes.conf.cpu().numpy()\n                    class_ids = result.boxes.cls.cpu().numpy().astype(int)\n                    \n                    # Format detections\n                    for box, score, cls in zip(boxes, scores, class_ids):\n                        # Filter for relevant classes (vehicles + people)\n                        if cls in [0, 1, 2, 3, 5, 6, 7, 8]:  # person, bicycle, car, motorcycle, bus, train, truck, boat\n                            detections.append([*box, score, cls])\n            \n            self.total_detections += len(detections)\n            return detections\n            \n        except Exception as e:\n            print(f\"Detection error: {e}\")\n            return []\n\n    def draw_tracks(self, frame, tracks):\n        \"\"\"Enhanced visualization of tracking results\"\"\"\n        annotated_frame = frame.copy()\n        \n        for track in tracks:\n            track_id = track['id']\n            bbox = track['bbox']\n            class_id = track['class_id']\n            conf = track['conf']\n            \n            x1, y1, x2, y2 = map(int, bbox)\n            \n            # Get color for this track\n            color = tuple(map(int, self.colors[track_id % len(self.colors)]))\n            \n            # Draw bounding box\n            cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n            \n            # Draw track ID and class\n            class_name = self.class_names[class_id] if class_id < len(self.class_names) else f\"object\"\n            label = f\"ID:{track_id} {class_name} {conf:.2f}\"\n            (text_width, text_height), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n            \n            # Draw text background\n            cv2.rectangle(annotated_frame, \n                         (x1, y1 - text_height - 10), \n                         (x1 + text_width + 10, y1), \n                         color, -1)\n            \n            # Put text\n            cv2.putText(annotated_frame, label, (x1 + 5, y1 - 5), \n                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)\n        \n        # Add frame info\n        cv2.putText(annotated_frame, f\"Frame: {self.total_frames}\", (10, 30),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n        cv2.putText(annotated_frame, f\"Active tracks: {len(tracks)}\", (10, 60),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n        \n        return annotated_frame\n\n    def process_video(self, input_path, output_path):\n        \"\"\"Process video with enhanced object tracking\"\"\"\n        print(f\"Processing video: {input_path}\")\n        \n        # Open video\n        cap = cv2.VideoCapture(input_path)\n        if not cap.isOpened():\n            print(f\"Error: Cannot open video {input_path}\")\n            return\n        \n        # Get video properties\n        fps = int(cap.get(cv2.CAP_PROP_FPS))\n        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        print(f\"Video properties: {width}x{height}, {fps} FPS, {total_frames} frames\")\n        \n        # Create output directory\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        \n        # Initialize video writer\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n        \n        self.total_frames = 0\n        self.total_detections = 0\n        \n        print(\"Starting object tracking...\")\n        \n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            self.total_frames += 1\n            \n            # Detect objects\n            detections = self.detect_objects(frame)\n            \n            # Track objects\n            tracks = self.tracker.update(detections, frame)\n            \n            # Draw tracking results\n            annotated_frame = self.draw_tracks(frame, tracks)\n            \n            # Write frame\n            out.write(annotated_frame)\n            \n            # Progress update\n            if self.total_frames % 30 == 0:\n                progress = (self.total_frames / total_frames) * 100\n                print(f\"Progress: {progress:.1f}% - Active tracks: {len(tracks)}\")\n        \n        # Release resources\n        cap.release()\n        out.release()\n        \n        print(f\"\\nTracking completed! Output saved to: {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:05:06.763872Z","iopub.execute_input":"2025-07-17T06:05:06.764469Z","iopub.status.idle":"2025-07-17T06:05:06.781547Z","shell.execute_reply.started":"2025-07-17T06:05:06.764442Z","shell.execute_reply":"2025-07-17T06:05:06.780818Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def main():\n    \"\"\"Main function to run enhanced object tracking\"\"\"\n    \n    # Define paths - UPDATE THESE TO YOUR PATHS\n    input_video = \"/kaggle/input/videofwces/MOT17-07-raw.webm\"  # Change this to your input video path\n    output_video = \"/kaggle/working/output/tracked_video.mp4\"  # Output path\n    \n    # Check if input video exists\n    if not os.path.exists(input_video):\n        print(f\"Error: Input video not found at {input_video}\")\n        print(\"Please update the input_video path in the main() function\")\n        return\n    \n    # Initialize tracker\n    print(\"Initializing object tracker...\")\n    tracker = ObjectTracker(\n        model_path='yolov8x.pt',  # Using smaller model for stability\n        conf_threshold=0.4,\n        iou_threshold=0.45\n    )\n    \n    # Process video\n    tracker.process_video(input_video, output_video)\n    \n    print(\"Tracking completed successfully!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:08:35.149624Z","iopub.execute_input":"2025-07-17T06:08:35.150394Z","iopub.status.idle":"2025-07-17T06:10:07.303633Z","shell.execute_reply.started":"2025-07-17T06:08:35.150366Z","shell.execute_reply":"2025-07-17T06:10:07.303008Z"}},"outputs":[{"name":"stdout","text":"Initializing object tracker...\nLoading YOLOv8 model...\n✓ Model loaded successfully. Device: cpu\nInitializing DeepOCSORT tracker...\n✓ Feature extractor initialized successfully\nProcessing video: /kaggle/input/videofwces/MOT17-07-raw.webm\nVideo properties: 960x540, 30 FPS, 500 frames\nStarting object tracking...\nProgress: 6.0% - Active tracks: 13\nProgress: 12.0% - Active tracks: 11\nProgress: 18.0% - Active tracks: 16\nProgress: 24.0% - Active tracks: 10\nProgress: 30.0% - Active tracks: 12\nProgress: 36.0% - Active tracks: 13\nProgress: 42.0% - Active tracks: 16\nProgress: 48.0% - Active tracks: 15\nProgress: 54.0% - Active tracks: 10\nProgress: 60.0% - Active tracks: 12\nProgress: 66.0% - Active tracks: 13\nProgress: 72.0% - Active tracks: 13\nProgress: 78.0% - Active tracks: 15\nProgress: 84.0% - Active tracks: 15\nProgress: 90.0% - Active tracks: 13\nProgress: 96.0% - Active tracks: 13\n\nTracking completed! Output saved to: /kaggle/working/output/tracked_video.mp4\nTracking completed successfully!\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}